import torch
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
from src.model.MalDetModel import MalDetModel


def test(
    model_path: str,
    test_dir: str,
    batch_size: int = 64,
):
    test = pd.read_csv(test_dir)  # Replace with actual file path
    test = test.apply(lambda x: pd.to_numeric(x, errors="coerce")).dropna()

    X_test = test.iloc[:, :-1].values
    y_test = test.iloc[:, -1].values

    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.long)

    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Initialize the model with the same structure
    input_size = X_test.shape[1]  # Adjust to match your dataset's input features
    model = MalDetModel(input_size)

    # Load the saved model parameters
    model.load_state_dict(torch.load(model_path, weights_only=True))

    # Set the model to evaluation mode (important for inference)
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    print(f"Accuracy on test set: {accuracy:.2f}%")
